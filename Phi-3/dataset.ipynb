{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset_opengen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_script_tags(text):\n",
    "    # Pattern to match <script> tags and their content\n",
    "    pattern = r'<script\\b[^<]*(?:(?!<\\/script>)<[^<]*)*<\\/script>'\n",
    "    # Replace the matched patterns with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return cleaned_text\n",
    "\n",
    "def parse_for_dataset(user_text, assistant_text):\n",
    "    # sys_prompt=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a Front End assistant, you will be prompted a description of web component/website and you should only return the code in HTML and TailwindCSS. DO NOT USE  HEADER TAGS OR ANY SCRIPT TAGS JUST THE HTML AND TAILWINDCSS CODE<|eot_id|>\"\n",
    "    sys_prompt=\"<s>\"\n",
    "    u_start=\"<|user|>\\n\"\n",
    "    u_end=\"<|end|>\\n\"\n",
    "    a_start=\"<|assistant|>\\n```html\"\n",
    "    a_end = \"```<|end|>\\n\"\n",
    "\n",
    "    result = sys_prompt+u_start+user_text+u_end + \\\n",
    "        a_start+remove_script_tags(assistant_text)+a_end\n",
    "    return {\"text\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset={}\n",
    "import json\n",
    "file_path = 'dataset_Phi-3.json'\n",
    "for index, row in df.iterrows():\n",
    "    u_prompt = row['u_desc']\n",
    "    code = row['code']\n",
    "    with open(file_path, 'a') as file:\n",
    "        \n",
    "        # Convert to JSON and append to the file\n",
    "        json_string = json.dumps(parse_for_dataset(u_prompt, code))\n",
    "        file.write(json_string + '\\n')  # Ensure newline for JSONL format\n",
    "    # Now you can use u_value and t_value as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def split_jsonl_file(original_file, first_file, second_file):\n",
    "    # Initialize counters\n",
    "    count = 0\n",
    "\n",
    "    # Open the first and second files for writing\n",
    "    with open(first_file, 'w') as file1, open(second_file, 'w') as file2:\n",
    "        # Step 1: Read the original .jsonl file\n",
    "        with open(original_file, 'r') as file:\n",
    "            for line in file:\n",
    "                # Parse the JSON object from the line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Step 2: Split the data and Step 3: Write to the appropriate .jsonl file\n",
    "                if count < 200:\n",
    "                    # Write to the first file\n",
    "                    json.dump(data, file1)\n",
    "                    file1.write('\\n')\n",
    "                else:\n",
    "                    # Write to the second file\n",
    "                    json.dump(data, file2)\n",
    "                    file2.write('\\n')\n",
    "\n",
    "                # Increment the counter\n",
    "                count += 1\n",
    "\n",
    "\n",
    "# Example usage\n",
    "split_jsonl_file('dataset_Phi-3.json', 'valid.json', 'train.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
